---
output: 
  pdf_document:
    keep_tex: true
header-includes:
    - \usepackage{setspace}\onehalfspacing
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(readxl)
library(tidyverse)
library(ggplot2)
library(dplyr)
library(hrbrthemes)
library(viridis)
library(zoo)
library(stringr)
library(ggrepel)
library(foreign)
library(AER)
library(lmtest)
library(sandwich)
library(texreg)
options(digits = 8)
options(scipen = 999)
rm(list=ls())
```

```{r, echo = F, include=F}
data = read.csv("data1.csv")
logsal = log(data$salary)
educ = data$educ
logbegin = data$salbegin
male = data$male
minority = data$minority
```


# Eksamenssæt 1

## Opgave 1 - Estimer modellen vha. OLS. Kommenter på outputtet og fortolk resultaterne
```{r}
model1 = lm(logsal ~ educ+logbegin+male+minority)
summary(model1)
```

Alle estimater er signifikante på 0,1% bortset fra "Minority", som er signifikant på 1%.

F-testen afvises grundet den lave p-værdi angivet ved < 0.000000000000000222, hvilket vil sige at variablene er "jointly significant", da nulhypotesen i F-testen, $H_0 = \beta_{1,2,3,4} = 0$ afvises. 
Under antagelse af, at alle andre variable er faste, vil en stigning i "educ" (uddannelse) på 1, medfører en stigning i lønnen på 3,5%. Et ekstra års uddannelse vil altså øge lønnen med 3,5%. Samme princip er gældende for de tre andre variable "logbegin", "male", "minority".
Derudover angiver ovenstående model at $R^2 = 0.77$, hvilket vil sige at dataen passer relativt godt til den estimerede model. 


## Opgave 2 - Udfør grafisk modelkontrol
For at udføre grafisk kontrol opstilles en række plots som beskrives nedenfor.

```{r}
par(mfrow=c(2,2))
plot(model1)
```
Ovenstående er udført grafisk modelkontrol. 
"Residuals vs. Fitted" viser at residualerne ikke er spredte, hvilket er indikation på at der ikke er tale om et "non-linear relationsship". Dog er den røde linje her næsten vandret (med undtagelse af den sidste del fra 4,5 på aksen for fitted values er linjens hældning negativ), hvilket kunne være tegn homoskedasticitet og dermed være tegn på et "linear relationship". 
"Q-Q plot" viser at residualerne tilnærmelsesvist følger en ret linje, og derfor antages formodes at være normalfordelt. 
"Scale-Location" belyser, at der er en skæv "scale-location", hvilket kan indikerer at der er tale om heteroskedasticitet for modellen. Ideelt ønskes, at den røde linje er vandret samt at residualpunkterne er spredt og tilfældigt fordelt. 
"Resudials vs. Leverage" tydeliggøre problematikken vedrørende outliers. Y-aksen angiver de standardiseret residualer givet ved $\frac {e}{se}$, mens x-aksen angiver viser "leverage" som måler hvor stor indflydelse en observation har på estimaternes for regressionens koefficienter. De stiblede linjer viser de forskellige niveauer for Cook's distance, som viser hvorvidt en outlier har stor betydning for estimationen for regressionens koefficienter. Hvis Cook's distancen for en observation er større end 1, anses den for at være indflydelsesrig for estimationen. I modellen ses tre outliers, 205, 343 og 029. Her ligger de to førstnævnte indenfor Cook's distance på 0,5 vurderes til at have lav indflydelse, mens 029 ligger på en Cook's distance imellem 0,5 og 1, hvilket heller ikke vurderes til at have den store effekt på regressionens estimater. 

Mangler noget med cook's distance - er det tilstrækkeligt? 
"Dog er den røde linje her næsten vandret, hvilket kunne være tegn på det modsatte."? - er det tilstrækkeligt?


## Opgave 3 - Test for heteroskedasticitet vha. Breusch-Pagan-testen og specialudgaven af White-testen
For at teste for heteroskedasticitet udføres BP-testen samt specialudgaven af White-testen. Her udføres både BP-testen både manuelt og vha. funktionen i R. 
BP-testen udføres ved at kvadrere fejlledet i den oprindelige regression, hvorefter denne opstilles som en funktion af de uafhængige variable fra den oprindelige regression. Der udføres en F-test eller LM-test for at estimere p-værdien, og hvis denne er under det valgte siginifikansniveau afvises nullhypotesen. 
White-testen udføres også ved at opstille en regression for det kvadreret fejlled. Dog opstilles denne med de uafængige variable, de kvadrerede uafhængige variable samt krydsprodukterne af de uafhængige variable. Igen udføres en F-test eller LM-test for at vurdere hvorvidt nulhypotesen afvises eller accepteres. 
SKAL KAN REDEGØRES FOR TESTEN?
$$H_0: homoskedasticitet$$
Hvis p-værdien er lav i BP-testen/F-test vil $H_0$ afvises hvorfor der antages at være heteroskedasticitet.
```{r}
u = resid(model1)
u2 = u^2
model1u = lm(u2 ~ educ+logbegin+male+minority) #Test for heteroskedasticitet
summary(model1u)
```
Da nulhypotesen i F-testen afvises angiver det, at der er heteroskedasticitet i modellen. Ved at bruge $\chi^2$ i stedet for F-test findes Breusch-Pagan testen.
```{r}
lm_chi = 0.077789*474 #BP-test
1-pchisq(lm_chi, 4) #p-værdien for chi-square


bptest(model1)
```

BP-værdien på 36.8719 indikerer, at variansen af residualerne ikke er konstant, hvilket også bekræftes ved den lave p-værdi for chi-square. Med den lave p-værdi, kan $H_0$ afvises, hvorfor det tyder på, at der findes heteroskedasticitet.

White-test med fitted værdier\\
Her benyttes residualer i anden som regresseres på de fitted værdier af modellen og de fitted værdier i anden for at belyse lineære og ikke-lineære forhold mellem de uafhængige variable og residualerne.
```{r}
white = lm(u2 ~ predict(model1) + I(predict(model1)^2))
summary(white)
```
Igen afvises nulhypotesen i F-testen og der antages derfor at være heteroskedasticitet. Både $\hat{y}$ og $\hat{y}^2$ er signifikante i testen, så det kan ikke siges hvorvidt der er et lineært eller ikke-lineært forhold.

## Opgave 4 - Beregn robuste standardfejl for modellen og sammenlign med resultaterne i spørgsmål 1
```{r}
model1robust <- coeftest(model1, vcov = vcovHC(model1, type = "HC0"))
screenreg(list(OLS = model1, OLS_robust_se = model1robust), digits = 4)
```
Den venstre kolonne udgør resultaterne fra opgave 1, mens den højre kolonne viser samme estimater men med robuste standardafvigelser. Der findes en lille ændring i signifikansniveauet, hvor "minority" er signifikant på et højere niveau, hvilket skyldes den lavere standardafvigelse. 

## Opgave 5 - Test hypotesen H0: $\beta_2 = 1$ mod alternativet H1: $\beta_2 \neq 1$
T-scoren beregnes med følgende formel
$$T = \frac{\hat{\beta}_j-\beta_j}{se(\hat{\beta}_j)}$$
Der vil blive brugt robust se \\
$a_j$ er H0
```{r}
#summary(model1)
t = (0.03008211-1) / 0.00296699
t

#kritiske værdier
alpha = c(0.05, 0.01)
qt(1-alpha/2, 469)

#P-værdier
pt(-abs(t), 469)
```
De kritiske værdier for 5\% og og 1\% er henholdsvis 1,96 og 2,59 hvorfor $H_0$ afvises og $\beta_2 \neq 1$. P-værdien rapporteres i R som 0, da t-scoren er så lav at p-værdien er for lav til at vise.


## Opgave 6 - Test hypotesen H0: $\beta_3 = \beta_4 = 0$
F-test
```{r}
linearHypothesis(model1, c("male=0", "minority=0"))
#model1a = lm(logsal ~ educ+logbegin)
#waldtest(model1, model1a)
```
RSS (kan også kaldes SSR) beskriver residual sum of squares, som er summen af de estimeret fejlled sat i anden, og hvis SSR = 0 er modellen perfekt ($R^2=1$).
Den meget lave p-værdi, som tilnærmelsesvist er nul, gør at H0 kan afvises. Dette betyder, at estimaterne "male" eller "minority" er "jointly significant" og dermed i fællesskab forskellig fra 0.

For at beregne F-statistic, kan der gøres brug af følgende formel: 
$$F = \frac {(SSR_r - SSR_{ur}) /q}{SSR_{ur} / (n - k - 1)}$$
Her angiver $r$ den begrænsede model, hvor $\beta_3 = \beta_4 = 0$, mens $ur$ angiver den ubegrænsede model hvor alle variable indgår. $q$ er forskellen i frihedsgrader mellem de to modeller. $(n-k-1)$ er antal frihedsgrader i den ubegrænset model, hvor $n$ er antal observationer og $k$ er antal variable i modellen.\\
Jf. ovenstående er F-værdien 22.6445, hvilket vil sige at der ikke er stor sandsynlighed for at de to estimater $\beta_3$ og $\beta_4$ er lig med nul. Derfor vil mindst et af de to estimater have relevans for modellen.

## Opgave 7 - Estimer modellen vha. FGLS og kommenter på resultaterne
```{r}
logu2 <- log(resid(model1)^2) #I do everything in one command, i.e., obtain resid, square them, and log them
varreg<-lm(logu2 ~ educ+logbegin+male+minority)
w <- exp(fitted(varreg))
model2fgls = lm(logsal ~ educ+logbegin+male+minority, weight=1/w)
summary(model2fgls)

bptest(model2fgls)
bptest(model1)
```
Det ses hvordan alle koefficienter er statistisk signifikante på 0,1%. Desuden har modellen en høj forklaringsgrad på $R^2 = 0,73$.
De relativt lave standard fejl i modellen indikerer præcise estimater. T-værdierne udgør alle høje værdier, og disse kan sammen med de lave p-værdier indikere, at den enkelte variable har en stærk effekt på den afhængige variable. 


## Opgave 8 - Har FGLS estimationen taget højde for al heteroskedasticiteten?

I ovenstående opgave 3 blev det tydeligt, at det signifikante resultat fra Breusch-Pagan testen indikerede, at modellen indeholdte heteroskedasticitet, hvilket er en god årsag til at benytte sig at FGLS estimationen. Denne tager nemlig højde for heteroskedasticitet i modellen, ved at omforme fejlledende så de bliver homoskedastiske (konstant varians). Anvendelsen af FGLS gør modellen mere præcis, end de resultater som opnås ved almindelig OLS, specielt når der er stærk heteroskedasticitet til stede. 
Det kan dog ikke udelukkes, at der efter FGLS estimationen ikke er mere heteroskedasticitet, og derfor kan der udeføres en BP-test eller White-test. 

```{r}
u2 = resid(model2fgls)^2
model2fgls_u = lm(u2 ~ educ+logbegin+male+minority, weight=1/w)
summary(model2fgls_u)
```
Da nulhypotesen i F-testen afvises angiver det, at der stadig er heteroskedasticitet i modellen trods brug af FGLS. Ved at bruge $\chi^2$ i stedet for F-test findes Breusch-Pagan testen.
```{r}
lm_chi = 0.070322*474 #BP-test
1-pchisq(lm_chi, 4) #p-værdien for chi-square
```
Det signifikante resultat fra Breusch-Pagan testen indikerer, at modellen stadig indeholder heteroskedasticitet.

```{r}
white_fgls = lm(u2 ~ predict(model2fgls) + I(predict(model2fgls)^2))
summary(white_fgls)
```
White-testen viser ligeledes tegn på heteroskedasticitet, da p-værdien i F-testen er tilnærmelsesvis nul.

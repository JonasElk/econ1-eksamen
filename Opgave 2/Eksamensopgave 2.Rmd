---
output: 
  pdf_document:
    keep_tex: true
header-includes:
    - \usepackage{setspace}\onehalfspacing
---

```{r setup2, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(readxl)
library(tidyverse)
library(ggplot2)
library(dplyr)
library(hrbrthemes)
library(viridis)
library(zoo)
library(stringr)
library(ggrepel)
library(foreign)
library(AER)
library(lmtest)
library(sandwich)
library(texreg)
library(tseries)
options(digits = 6)
options(scipen = 999)
rm(list=ls())
```

```{r, echo = F, include=F}
data2 = read.csv("data2.csv")
```

# Eksamenssæt 2

## Opgave 1 - Estimer de to modeller vha. OLS. Kommenter på outputtet, sammenlign og fortolk resultaterne.

For at estimere modellen vha. OLS, opstilles der en regression vha. den indbyggede funktion i R. Målet med en OLS-regression er at minimere "sum of squared residuals, SSR" givet ved 
$$SSR = \sum_{i=n}^{n}(\hat{u}_i)^2$$ 
Jo større SSR, des mindre passer dataen til den estimeret model. Dermed vil man med en OLS-regression minimere følgende funktion.
$$min_\beta\sum_{i=n}^{n}(y_i-\hat{y_i})$$ 
Hvor $y_i$ er den observerede værdi og $\hat{y_i}$ er den estimerede værdi af modellen.

For model 1 vil der være en level-level fortolkning på estimaterne, mens der vil være en log-level fortolkning af estimaterne for model 2, med undtagelse af estimatet "log(salbegin)", som vil blive fortolket ved log-log.

```{r}
model1 = lm(salary ~ educ + salbegin + male + minority, data = data2)
model2 = lm(log(salary) ~ educ + log(salbegin) + male + minority, data = data2)

screenreg(list(model1, model2), digits = 4)
```

Model 1 og model 2 er forskellige fra hinanden idet variablen "salbegin" inddrages i model 1, mens variablen "log(salbegin)" er inddraget i model 2.

For model 1 er alle estimaterne signifikante med undtagelse af "minority", hvilket betyder, at det ikke er sikkert, at den har en betydning for den afhængige variabel. Skæringspunktet ("intercept"), "educ" og "salbegin" er signifikante på 0.1%, mens "male" er signifikant på 5% og "minority ikke er signifikant.

For model 2 er alle estimaterne signifikante, hvor skæringspunktet ("intercept"), "educ" og "log(salbegin)" er signifikante på 0.1%, mens estimaterne "male" og "minority" er signifikante på 5%.

Estimaterne i model 1 angiver, at en stigning i den enkelte variabel (under antagelse af, at de resterende variable holdes konstant) vil medføre en ændring i lønnen på det tilsvarende estimat.

Estimaterne (med undtagelse af "log(salbegin)") i model 2 angiver at ved en enheds stigning i den pågældende koeffiecient (under antagelse, at de resterende variable holdes konstant) vil medføre en procentvis stigning/et procentvist fald (approksimeret) i lønnen, angivet ved "log(salary)". Derudover angiver "log(salbegin)" at ved en 1% stigning (under antagelse at de resterende variable holdes konstant) i begyndelseslønnen vil medføre en approksimeret stigning på 0,82% i lønnen.

$R^2$ angiver forklaringsgraden af dataen i forhold til den estimeret model. Her kan det ses at $R^2 = 0.80$ i model 1, mens $R^2 = 0.81$ for model 2. Kigges der i stedet på "adjusted $R^2$", som også tager hensyn til hvorvidt flere variable tilføjes til modellen, kan det ses at model 1 har en $R^2 = 0.79$ mens model 2 har en $R^2 = 0.8$. Dermed uanset hvilken $R^2$ der tages udgangspunkt i passer dataen til model 2 1% bedre til modellen sammenlignet med model 1.

## Opgave 2 - Udfør grafisk modelkontrol af de to modeller. Hvilken model vil du foretrække?

For at udføre grafisk modelkontrol af de to modeller opstilles tre plots, som på forskellige måder illustrere hvordan regressionen for de to modeller.

```{r}
par(mfrow = c(1,2))
plot(model1, 2)
hist(rstandard(model1), prob=T, col = rgb(0.8,0.8,1), ylim=c(0,0.6))
lines(density(rstandard(model1)))

plot(model2, 2)
hist(rstandard(model2), prob=T, col = rgb(0.8,0.8,1), ylim=c(0,0.45))
lines(density(rstandard(model2)))

#Grafisk kontrol for misspecification
plot(model1, 1)
plot(model2, 1)
```

I ovenstående kan der ses tre plot for hver af de to modeller: "Q-Q residuals", histogram for de standardiserede residualer samt plot for "residuals vs. fitted"

For Q-Q residuals følger punkterne den stiplede rette linje. Hvis punkterne systematisk afviger fra den rette linje vil det tyde på, at residualerne ikke er normalfordelte. For model 1 passer punkterne til dels den rette linje. Dog er der også en del outliers især til højre for 0 på x-aksen. Modsat for model 2 passer de standardiserede residualer noget bedre til den rette linje. Dog er der en svag systematisk afvigelse i yderpunkterne, hvor punkterne ligger over den stiplede linje.

Histogrammet for standardiserede residualer vil være normalfordelt, hvis middelværdien for residualerne ligger omkring nul, hvilket vil indikere, at regressionens fejlled ikke over- eller undervurderer modellen systematisk. For model 1 ser fordelingen ud til at være en smule højreskæv, dog med en middelværdi på omkring 0. Samme tendens ses for model 2, hvor fordelingen her også er højreskæv, dog ligger middelværdien her en smule til venstre for nul.

For "Residuals vs. fitted" vil det ses, at punkterne er tilfældigt spredt over en vandret rød linje, hvis der er et lineært forhold mellem de uafhængige og afhængige variable. Residualerne for model 1 er ikke spredt, og den røde linje har en faldende tendens. Dog jo længere ud på x-aksen vi kommer jo mere tilfældigt spredte er de. Model 2 har samme tendens, hvor det dog ser ud til at punkterne er mere spredte (her er det vigtigt at være obs på x-aksens værdier, hvor x-aksen kun strækker sig fra 3-5 for model 2, mens den strækker sig fra 20-120 i model 1 - hvorfor det kan linje at model 1 er mindre spredt ift. model 2). Den røde linje for model 2 er dog fladere sammenlignet med model 1.

## Opgave 3 - Undersøg om de to modeller er misspecificerede vha. RESET-testet

RESET står for "regression specification error test" og blev lavet af Ramsey i 1969. Hvis en model opfylder MLR4 (homoskedasticitet), kan man teste for misspecifikation, ved at tilføje de uafhængige variable kvadreret og i tredje, og derefter udføre en F-test for at teste for "joint significance". De kvadrerede variable og i tredjepotens vil fange, hvis der er noget ikke-lineært i modellen, og derfor vil de tilføjede variable være insignifikante for modellen, såfremt modellen ikke er misspecificeret.

```{r}
y2 = fitted(model1)^2
y3 = fitted(model1)^3

reset1 = lm(salary ~ educ + salbegin + male + minority + y2 + y3, data = data2)

linearHypothesis(reset1, c("y2=0", "y3=0"))

#resettest(model1)
```

```{r}
y2 = fitted(model2)^2
y3 = fitted(model2)^3
reset2 = lm(log(salary) ~ educ + log(salbegin) + male + minority + y2 + y3, data = data2)
linearHypothesis(reset2, c("y2=0", "y3=0"))

#resettest(model2)
```

I begge modeller er det kvadrerede led samt leddet i tredje insignifikante på 5%, men signifikante på 10%. De er dermed misspecificeret på 10% men ikke 5%. Hvorvidt den er misspecificeret ud fra reset-test afhænger derfor af signifikansniveauet.

## Opgave 4 - Forklar hvorfor det kunne være relevant at medtage $educ2$ som forklarende variabel i de to modeller. Estimer de to modeller igen hvor $educ2$ inkluderes (med tilhørende koefficient $\beta_5$), kommenter kort på outputtet og udfør RESET-testet igen.

Som det fremgår af 2.3, kan modellen være misspecificeret. Det kan derfor efterprøves, om det skyldes en ikke-lineær sammenhæng mellem længden af uddannelse (educ) og løn, ved at inkludere $educ^2$, for derefter lave RESET igen og se om der herefter vil være et klart svar på, hvorvidt modellen er misspecifiseret.

```{r}
educ2 = data2$educ^2
model1educ = lm(salary ~ educ + educ2 + salbegin + male + minority, data = data2)
model2educ = lm(log(salary) ~ educ + educ2 + log(salbegin) + male + minority, data = data2)

screenreg(list("Model1" = model1, "Model 1 m. educ2"= model1educ, "Model 2" = model2, "Model 2 m. educ2" = model2educ), digits = 4)
```

Der ses en relativ stor forskel i estimaterne, når $educ^2$ tages med i modellen. Eksempelvis er estimatet for $educ$ ændret fra ca. 1 til -2.3 i model 1. Dette kan være tegn på, at udeladelsen af det kvadrerede led har skabt en relativ stor bias i estimatet.

```{r}
reset(model1educ)
reset(model2educ)
```

Der er højere p-værdier i begge, hvorfor de nu begge er insignifikante på 15% og under. Dermed er begge modeller nu ikke misspecificeret på noget relevant signifikansniveau. Dog er hverken $educ$ eller $educ^2$ signifikante på 5% eller lavere i model 2.

## Opgave 5 - Test hypotesen $H0 : \beta_1 = \beta_5 = 0$ i begge modeller (fra spørgsmål 4).

F-testen benyttes ofte til at sammenligne varians mellem flere grupper. F-test kan også bruge til at test om flere koefficienter er lig nul. I dette tilfælde opstilles den ovenstående nulhypotese. Følgende formel bruges til, at konkludere hvorvidt nulhypotesen afvises eller ej: $$F = \frac {(SSR_r - SSR_{ur}) /q}{SSR_{ur} / (n - k - 1)}$$

```{r}
linearHypothesis(model1educ, c("educ=0", "educ2=0"))
#model1a = lm(logsal ~ educ+logbegin)
#waldtest(model1, model1a)
```

I dette tilfælde er p-værdien ekstremt lille, og derfor forkastes nulhypotesen. Desuden indikerer den høje F-værdi på 23.56, at de to variable "educ" og "educ2" har signifikant indflydelse på lønnen, som i dette tilfælde er den afhængeige variable.

```{r}
linearHypothesis(model2educ, c("educ=0", "educ2=0"))
#model1a = lm(logsal ~ educ+logbegin)
#waldtest(model1, model1a)
```

Også ved denne model ses en lav p-værdi, og derfor forkastes nulhypotesen. Igen indikere den høje F-værdi på 18.94, at variablene "educ" og "educ2" har signifikant indflydelse på log løn. Dog er F-værdien i dette tilfælde mindre end ovenfor.

## Opgave 6 - Kunne der være problemer med målefejl i de to modeller? I hvilke tilfælde vil det udgøre et problem?

Hvis den afhængige variabel, i dette tilfælde lønnen, har problemer med målefejl, vil dette komme til udtryk i fejlleddet i regressionen. Da lønnen, i tilfælde af målefejl, er den faktiske løn fratrukket dets fejlled, vil fejlleddet i regressionen være summen af det oprindelige fejlled, u, og målefejlen $e_0$. Det vil ikke skabe et problem så længe det oprindelige fejlled og målefejlen ikke er korreleret med nogle af de uafhængige variable, da det ellers vil skabe en bias i estimaterne.

Hvis den uafhængige variabel derimod har målefejl afhænger effekten af antagelserne vedrørende målefejlen. I dette tilfælde kunne der potentielt være en målefejl i variablen educ, da den kan dække over meget forskellig uddannelse som kan være svært at opgøre. Hvis der ikke er en korrelation mellem målefejlen og den observerede variabel, dvs. $cov(educ,e_{educ}) = 0$, hvor $e_{educ}$ er målefejlen i variablen, er der nødvendigvis en korrelation mellem målefejlen og den ikke-observerede variabel $cov(educ^*,e_{educ}) \neq 0$. Derfor bliver fejlleddet i regressionen det oprindelige fejlled, u, fratrukket $\beta_1e_{educ}$. I dette tilfælde vil estimatet af regressionen stadig være unbiased og consistent, da fejlleddet ikke er korreleret med den observerede variabel.

Hvis der er en korrelation mellem målefejlen og den observerede variabel $cov(educ,e_{educ}) \neq 0$, da opstår fejlen CEV (classical errors-in variables). Dette betyder estimatoren er biased, og OLS altid vil undervurdere effekten af $\beta$. Da det er en multipel regression, vil en målefejl i dette tilfælde i en afhængig variabel betyde bias i alle andre parametre.
